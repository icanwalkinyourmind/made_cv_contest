{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import tqdm\n",
    "from torch.nn import functional as fnn\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from hack_utils import NUM_PTS, CROP_SIZE\n",
    "from hack_utils import ScaleMinSideToSize, CropCenter, TransformByKeys, RandomRotation90\n",
    "from hack_utils import ThousandLandmarksDataset\n",
    "from hack_utils import restore_landmarks_batch, create_submission\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# refactor functions from hack_train\n",
    "\n",
    "def train(model, loader, loss_fn, optimizer, device):\n",
    "    model.train()\n",
    "    train_loss_wing = []\n",
    "    train_loss_mse = []\n",
    "    for batch in tqdm.notebook.tqdm(loader, total=len(loader), desc=\"training...\"):\n",
    "        images = batch[\"image\"].cuda()  # B x 3 x CROP_SIZE x CROP_SIZE\n",
    "        landmarks = batch[\"landmarks\"] # B x (2 * NUM_PTS)\n",
    "\n",
    "        pred_landmarks = model(images).cpu() # B x (2 * NUM_PTS)\n",
    "        \n",
    "        loss = loss_fn[1](pred_landmarks, landmarks)\n",
    "        train_loss_wing.append(loss.item())\n",
    "        \n",
    "        # tried both loss, and there combination as main loss function, but mse showing better results\n",
    "        loss = loss_fn[0](pred_landmarks, landmarks)\n",
    "        train_loss_mse.append(loss.item())\n",
    "        \n",
    "      \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Train loss: wing_loss = {np.mean(train_loss_wing)}')\n",
    "    return np.mean(train_loss_mse)\n",
    "\n",
    "\n",
    "def validate(model, loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    val_loss_wing = []\n",
    "    val_loss_mse = []\n",
    "    for batch in tqdm.notebook.tqdm(loader, total=len(loader), desc=\"validation...\"):\n",
    "        images = batch[\"image\"].cuda()\n",
    "        landmarks = batch[\"landmarks\"]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred_landmarks = model(images).cpu()\n",
    "            \n",
    "        loss = loss_fn[0](pred_landmarks, landmarks)\n",
    "        val_loss_mse.append(loss.item())\n",
    "        \n",
    "        loss = loss_fn[1](pred_landmarks, landmarks)\n",
    "        val_loss_wing.append(loss.item())\n",
    "        \n",
    "    print(f'Val loss: wing_loss = {np.mean(val_loss_wing)}')\n",
    "    return np.mean(val_loss_mse)\n",
    "\n",
    "\n",
    "def predict(model, loader, device):\n",
    "    model.eval()\n",
    "    predictions = np.zeros((len(loader.dataset), NUM_PTS, 2))\n",
    "    for i, batch in enumerate(tqdm.notebook.tqdm(loader, total=len(loader), desc=\"test prediction...\")):\n",
    "        images = batch[\"image\"].cuda()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred_landmarks = model(images).cpu()\n",
    "            \n",
    "        pred_landmarks = pred_landmarks.numpy().reshape((len(pred_landmarks), NUM_PTS, 2))  # B x NUM_PTS x 2\n",
    "\n",
    "        fs = batch[\"scale_coef\"].numpy()  # B\n",
    "        margins_x = batch[\"crop_margin_x\"].numpy()  # B\n",
    "        margins_y = batch[\"crop_margin_y\"].numpy()  # B\n",
    "        prediction = restore_landmarks_batch(pred_landmarks, fs, margins_x, margins_y)  # B x NUM_PTS x 2\n",
    "        predictions[i * loader.batch_size: (i + 1) * loader.batch_size] = prediction\n",
    "\n",
    "    return predictions\n",
    "\n",
    "    \n",
    "\n",
    "data_path = './data/'\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and add transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "315144it [06:59, 751.40it/s]\n",
      "393931it [01:45, 3720.42it/s]  \n"
     ]
    }
   ],
   "source": [
    "import albumentations as albu\n",
    "# 1. prepare data & models\n",
    "train_augmentations = albu.Compose([\n",
    "    albu.HueSaturationValue(\n",
    "                    hue_shift_limit=10,\n",
    "                    sat_shift_limit=20,\n",
    "                    val_shift_limit=0,\n",
    "                    p=0.3),\n",
    "    albu.RandomBrightnessContrast(\n",
    "                    brightness_limit=0.3,\n",
    "                    contrast_limit=0.3,\n",
    "                    p=0.3)])\n",
    "train_transforms = transforms.Compose([\n",
    "    RandomRotation90(0), # do not rotatet, lower score with 90 degree rotation \n",
    "    ScaleMinSideToSize((CROP_SIZE, CROP_SIZE)),\n",
    "    CropCenter(CROP_SIZE),\n",
    "    TransformByKeys(transforms.ToPILImage(), (\"image\",)),\n",
    "    TransformByKeys(transforms.ToTensor(), (\"image\",)),\n",
    "    TransformByKeys(transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]), (\"image\",)),\n",
    "    \n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    RandomRotation90(0),\n",
    "    ScaleMinSideToSize((CROP_SIZE, CROP_SIZE)),\n",
    "    CropCenter(CROP_SIZE),\n",
    "    TransformByKeys(transforms.ToPILImage(), (\"image\",)),\n",
    "    TransformByKeys(transforms.ToTensor(), (\"image\",)),\n",
    "    TransformByKeys(transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]), (\"image\",)),\n",
    "    \n",
    "])\n",
    "\n",
    "print(\"Reading data...\")\n",
    "train_dataset = ThousandLandmarksDataset(os.path.join(data_path, 'train'), train_augmentations, train_transforms, split=\"train\")\n",
    "\n",
    "val_dataset = ThousandLandmarksDataset(os.path.join(data_path, 'train'), None, val_transforms, split=\"val\")\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "num_workers=4\n",
    "train_dataloader = data.DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers, pin_memory=True,\n",
    "                                   shuffle=True, drop_last=True)\n",
    "val_dataloader = data.DataLoader(val_dataset, batch_size=batch_size, num_workers=num_workers, pin_memory=True,\n",
    "                                 shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coord_conv import CoordConv\n",
    "import segmentation_models_pytorch as smp\n",
    "class Convert(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(x.shape[0], -1)\n",
    "            \n",
    "class LinearHead(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, activation=None, upsampling=1):\n",
    "        super().__init__()\n",
    "        # also tried to use CoordConv layer\n",
    "        self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size // 2)\n",
    "        self.convert = Convert()\n",
    "        self.linear = nn.Linear(CROP_SIZE**2, 2 * NUM_PTS, bias=True)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        res = {}\n",
    "        x = self.conv2d(x)\n",
    "        out = self.convert(x)\n",
    "        out = self.linear(out)\n",
    "        return  out\n",
    "        \n",
    "        \n",
    "    \n",
    "ENCODER = 'vgg16_bn'\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "CLASSES = ['0']\n",
    "ACTIVATION = 'sigmoid'\n",
    "\n",
    "model = smp.Unet(\n",
    "    encoder_name=ENCODER, \n",
    "    encoder_weights=ENCODER_WEIGHTS, \n",
    "    classes=1, \n",
    "    activation=ACTIVATION,\n",
    ").cuda()\n",
    "\n",
    "# change segmentation head with custom linear head\n",
    "model.segmentation_head = LinearHead(16, 1)\n",
    "model.cuda(0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wingloss implementation form github\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "# torch.log  and math.log is e based\n",
    "class WingLoss(nn.Module):\n",
    "    def __init__(self, omega=10, epsilon=2):\n",
    "        super(WingLoss, self).__init__()\n",
    "        self.omega = omega\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        y = target\n",
    "        y_hat = pred\n",
    "        delta_y = (y - y_hat).abs()\n",
    "        delta_y1 = delta_y[delta_y < self.omega]\n",
    "        delta_y2 = delta_y[delta_y >= self.omega]\n",
    "        loss1 = self.omega * torch.log(1 + delta_y1 / self.epsilon)\n",
    "        C = self.omega - self.omega * math.log(1 + self.omega / self.epsilon)\n",
    "        loss2 = delta_y2 - C\n",
    "        return (loss1.sum() + loss2.sum()) / (len(loss1) + len(loss2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda: 0'\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4, amsgrad=True)\n",
    "loss_fn = fnn.mse_loss\n",
    "losses = [loss_fn, WingLoss(5, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready for training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f5c506bd61c4c6da63e059044baf8ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='training...', max=2462.0, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2. train & validate\n",
    "n_epoch = 50\n",
    "model.train()\n",
    "print(\"Ready for training...\")\n",
    "best_val_loss = np.inf\n",
    "for epoch in range(n_epoch):\n",
    "    train_loss = train(model, train_dataloader, losses, optimizer, device=device)\n",
    "    val_loss = validate(model, val_dataloader, losses, device=device)\n",
    "    print(\"Epoch #{:2}:\\ttrain loss: {:5.4}\\tval loss: {:5.4}\".format(epoch, train_loss, val_loss))\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        with open(f\"vgg16_bn_unet_best_mse.pth\", \"wb\") as fp:\n",
    "            torch.save(model.state_dict(), fp) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in val_dataloader:\n",
    "    break\n",
    "    \n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    _, x = model(b['image'][:10, ...].cuda())\n",
    "\n",
    "from matplotlib.patches import Circle\n",
    "def plot_img(img, points):\n",
    "    fig,ax = plt.subplots(1)\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    # Show the image\n",
    "    ax.imshow(img.numpy().transpose(1,2,0))\n",
    "\n",
    "    # Now, loop through coord arrays, and create a circle at each x,y pair\n",
    "    for xx,yy in points:\n",
    "        circ = Circle((xx,yy),0.5)\n",
    "        ax.add_patch(circ)\n",
    "\n",
    "    # Show the image\n",
    "    plt.show()\n",
    "n = 10\n",
    "i = 0\n",
    "for i, (img, pred) in enumerate(zip(b['image'], x)):\n",
    "    if i >= n:\n",
    "        break\n",
    "    print('Predicted landmarks')\n",
    "    plot_img(img, pred.cpu().numpy().reshape(1942//2, 2))\n",
    "    print('True landmarks')\n",
    "    plot_img(img, b['landmarks'][i].numpy().reshape(1942//2, 2))\n",
    "    print('-'*40)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "99820it [00:00, 436308.85it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7e584a204e84d0eb5aa808d4e91f4fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='test prediction...', max=780.0, style=ProgressStyle(descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "test_transforms = transforms.Compose([\n",
    "    ScaleMinSideToSize((CROP_SIZE, CROP_SIZE)),\n",
    "    CropCenter(CROP_SIZE),\n",
    "    TransformByKeys(transforms.ToPILImage(), (\"image\",)),\n",
    "    TransformByKeys(transforms.ToTensor(), (\"image\",)),\n",
    "    TransformByKeys(transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]), (\"image\",)),\n",
    "    \n",
    "])\n",
    "test_dataset = ThousandLandmarksDataset(os.path.join(data_path, 'test'), None, test_transforms, split=\"test\")\n",
    "test_dataloader = data.DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers, pin_memory=True,\n",
    "                                  shuffle=False, drop_last=False)\n",
    "\n",
    "with open(f\"vgg16_bn_unet_best_mse.pth\", \"rb\") as fp:\n",
    "    best_state_dict = torch.load(fp, map_location=\"cpu\")\n",
    "    model.load_state_dict(best_state_dict)\n",
    "\n",
    "test_predictions = predict(model, test_dataloader, device)\n",
    "\n",
    "create_submission(data_path, test_predictions, f\"vgg16_unet_submit.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mail_cv",
   "language": "python",
   "name": "mail_cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
